{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement tensorflow==1.7.1 (from unityagents==0.4.0) (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 2.0.0a0)\n",
      "No matching distribution found for tensorflow==1.7.1 (from unityagents==0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "# %load_ext version_information\n",
    "# %version_information numpy, unityagents, torch, matplotlib, pandas, statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "path = r\"C:\\Users\\adith\\Desktop\\Meng Robotics\\reinforcement\\Banana\\MADDPG\\Multiagent\\Tennis_Windows_x86_64\\Tennis\"\n",
    "env = UnityEnvironment(file_name=path , seed =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.63803244 -1.5\n",
      "  -0.          0.          6.00063038  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -7.07908773 -1.5\n",
      "   0.          0.         -6.00063038  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states [[-0.31058237 -0.39191814]\n",
      " [-1.          0.01878997]]\n",
      "states [[ 0.02239289 -1.        ]\n",
      " [-0.61703368 -0.74990251]]\n",
      "states [[-1.          1.        ]\n",
      " [-0.36885429  0.24816117]]\n",
      "states [[ 1.         -0.05274805]\n",
      " [ 1.         -0.12518188]]\n",
      "states [[-0.32776779 -1.        ]\n",
      " [ 0.64477366  0.30933077]]\n",
      "states [[ 1.          0.54520293]\n",
      " [-1.          0.90649638]]\n",
      "states [[ 0.381693    0.10928335]\n",
      " [-1.          0.23752813]]\n",
      "states [[ 1.         -0.48667054]\n",
      " [ 0.69060897 -1.        ]]\n",
      "states [[-1.          0.86410558]\n",
      " [-0.48776428  1.        ]]\n",
      "states [[ 0.8464251  -0.82421916]\n",
      " [ 0.27196171 -1.        ]]\n",
      "states [[-0.54978574 -1.        ]\n",
      " [ 0.02753759  1.        ]]\n",
      "states [[ 0.38246044  0.7778487 ]\n",
      " [-1.          1.        ]]\n",
      "states [[-0.79664282  1.        ]\n",
      " [-0.00842017  0.24378762]]\n",
      "states [[-1.         -0.76075501]\n",
      " [ 0.10657361 -0.11973531]]\n",
      "states [[ 0.80925237 -0.91850122]\n",
      " [-1.          0.64756776]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "states [[-1.          1.        ]\n",
      " [ 0.9327925   0.29164906]]\n",
      "states [[-1. -1.]\n",
      " [-1. -1.]]\n",
      "states [[-1.         -0.09707133]\n",
      " [-1.         -0.50826329]]\n",
      "states [[-0.64179384 -1.        ]\n",
      " [-0.16347065 -1.        ]]\n",
      "states [[-1.         -0.42055401]\n",
      " [ 0.14171393 -1.        ]]\n",
      "states [[-0.48291651 -1.        ]\n",
      " [-0.93071127  1.        ]]\n",
      "states [[-0.11326697 -1.        ]\n",
      " [ 0.81481885  0.50187717]]\n",
      "states [[ 0.90219258 -0.97327098]\n",
      " [ 0.26120056 -1.        ]]\n",
      "states [[-0.42398727 -1.        ]\n",
      " [ 0.38162915  0.14151556]]\n",
      "states [[ 1.         -0.43696063]\n",
      " [-1.          0.53458258]]\n",
      "states [[-0.33583851 -0.43264693]\n",
      " [-0.57549318  0.87866366]]\n",
      "states [[ 1.         -1.        ]\n",
      " [ 0.21930835 -0.01809096]]\n",
      "states [[-0.56237111  0.14446328]\n",
      " [-0.39055044  0.25564024]]\n",
      "states [[-0.05785343  0.01357756]\n",
      " [ 0.61790739 -0.7408923 ]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "states [[-1.          0.64756212]\n",
      " [ 0.7300613   0.32894627]]\n",
      "states [[ 1.  1.]\n",
      " [ 1. -1.]]\n",
      "states [[-0.16081705 -1.        ]\n",
      " [ 0.90704831  1.        ]]\n",
      "states [[ 0.77806752 -0.81962376]\n",
      " [ 0.17821516  0.09054704]]\n",
      "states [[-1.         -0.6630152 ]\n",
      " [ 0.22348224 -0.60478596]]\n",
      "states [[-1.         -0.49440851]\n",
      " [ 1.          0.31788084]]\n",
      "states [[-0.41290839 -0.83996742]\n",
      " [ 1.          1.        ]]\n",
      "states [[-0.53671327  0.23263821]\n",
      " [-1.          0.33673567]]\n",
      "states [[-1.          1.        ]\n",
      " [-0.10101708  0.72758267]]\n",
      "states [[ 0.17051537  1.        ]\n",
      " [-1.          1.        ]]\n",
      "states [[ 1.         -0.24978976]\n",
      " [ 0.11698342 -0.99962253]]\n",
      "states [[ 0.88072624  0.25551866]\n",
      " [-0.84604203  0.86282783]]\n",
      "states [[ 0.03261615 -0.08581926]\n",
      " [-0.51648646  0.08087752]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "states [[ 0.83769782  0.36613723]\n",
      " [-1.          1.        ]]\n",
      "states [[-0.13495368 -0.60678076]\n",
      " [ 1.         -0.34570543]]\n",
      "states [[-0.04712793 -1.        ]\n",
      " [ 1.          1.        ]]\n",
      "states [[ 1.        -1.       ]\n",
      " [-0.1896189  1.       ]]\n",
      "states [[-1.          0.44439292]\n",
      " [ 0.60052917 -1.        ]]\n",
      "states [[-0.4203767   0.06207582]\n",
      " [ 0.50066259  0.1600614 ]]\n",
      "states [[-1.          0.0852858 ]\n",
      " [ 0.58767399  1.        ]]\n",
      "states [[ 0.66337941  0.11169418]\n",
      " [ 0.08184544 -0.55534346]]\n",
      "states [[-0.08530145  0.51289676]\n",
      " [ 0.52297729 -1.        ]]\n",
      "states [[-0.59580877  0.86722754]\n",
      " [ 0.12960337 -0.87740687]]\n",
      "states [[ 0.88695368  0.91850568]\n",
      " [-0.41734233  1.        ]]\n",
      "states [[-1.         -0.99468802]\n",
      " [-0.05271108 -0.12262943]]\n",
      "states [[-0.94795555 -1.        ]\n",
      " [-1.          0.37840845]]\n",
      "states [[ 0.38570342 -0.59367186]\n",
      " [ 0.23027247 -0.16557446]]\n",
      "states [[ 1.          0.76062411]\n",
      " [ 0.81068889 -0.89429287]]\n",
      "states [[-0.04726998 -0.59625238]\n",
      " [-1.          0.2585394 ]]\n",
      "states [[ 0.18469263  0.29848524]\n",
      " [ 1.         -1.        ]]\n",
      "states [[-0.51670077 -0.56438204]\n",
      " [-0.13731398  1.        ]]\n",
      "states [[ 0.29993437 -0.44844097]\n",
      " [ 1.         -0.02569717]]\n",
      "states [[-0.36246446  0.09263258]\n",
      " [ 0.76991494  0.89196805]]\n",
      "states [[ 0.65089544  0.81863511]\n",
      " [-0.31085373 -0.19729904]]\n",
      "states [[-0.74454515 -0.23377578]\n",
      " [ 0.76946191  0.27863658]]\n",
      "states [[ 1.          0.5059247 ]\n",
      " [-0.12098824 -1.        ]]\n",
      "states [[0.16685126 0.65685424]\n",
      " [0.15409486 0.68166608]]\n",
      "states [[ 0.2757526   1.        ]\n",
      " [ 0.08301589 -0.09619319]]\n",
      "states [[ 0.44620088 -0.12719925]\n",
      " [ 1.          0.74225396]]\n",
      "states [[-1. -1.]\n",
      " [-1.  1.]]\n",
      "states [[-0.77259574 -1.        ]\n",
      " [ 1.         -1.        ]]\n",
      "states [[-0.09028622  0.68639127]\n",
      " [ 0.59418806 -0.53038804]]\n",
      "states [[-0.39297485  1.        ]\n",
      " [-0.10772747 -0.02304432]]\n",
      "states [[-0.1858769   1.        ]\n",
      " [-0.44543642  0.82617496]]\n",
      "states [[0.34198121 1.        ]\n",
      " [0.15432031 0.43037143]]\n",
      "states [[-0.63545002  0.25344335]\n",
      " [ 1.         -0.66391686]]\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "states [[-1.         -1.        ]\n",
      " [-0.81187496  0.27525074]]\n",
      "states [[ 1.         -0.80080879]\n",
      " [-0.42493247 -1.        ]]\n",
      "states [[-0.15833686  0.00245499]\n",
      " [-1.         -1.        ]]\n",
      "states [[ 0.48613586 -0.14682531]\n",
      " [-0.08145214  0.62823826]]\n",
      "states [[-1.         -1.        ]\n",
      " [-1.          0.40416506]]\n",
      "states [[ 0.78075009 -1.        ]\n",
      " [ 0.55608897 -0.26865774]]\n",
      "states [[-0.78166396 -0.79424763]\n",
      " [ 0.64897224 -0.00210151]]\n",
      "states [[-0.20803316  0.91925642]\n",
      " [ 0.93918809  0.61720027]]\n",
      "states [[ 1.         -0.30295663]\n",
      " [ 0.15607528 -1.        ]]\n",
      "states [[ 0.81495767 -0.20072862]\n",
      " [-0.50460344  0.21357027]]\n",
      "states [[ 0.97783851 -1.        ]\n",
      " [-0.48889839 -0.29694117]]\n",
      "states [[-0.76367727 -1.        ]\n",
      " [-0.42861262  1.        ]]\n",
      "states [[-0.42331066 -0.97934668]\n",
      " [-0.87508971 -0.62588282]]\n",
      "states [[ 0.55845447 -1.        ]\n",
      " [-0.86572725 -0.20709173]]\n",
      "states [[ 1.          0.28955146]\n",
      " [-0.795948   -0.3623822 ]]\n",
      "states [[-0.27414452  0.03659616]\n",
      " [-1.         -0.10163987]]\n",
      "states [[-0.42162212  1.        ]\n",
      " [-0.48552641  1.        ]]\n",
      "states [[ 0.06457704  1.        ]\n",
      " [-0.84201589 -1.        ]]\n",
      "states [[-1.         -0.76812024]\n",
      " [-1.          1.        ]]\n",
      "states [[ 0.78890607  0.89821735]\n",
      " [ 1.         -1.        ]]\n",
      "states [[-0.12729924  0.49673948]\n",
      " [-1.         -0.23571805]]\n",
      "states [[0.13204174 0.97697435]\n",
      " [0.65153267 0.10763182]]\n",
      "states [[-0.83459162  0.48015965]\n",
      " [ 1.          1.        ]]\n",
      "states [[-0.76707305  1.        ]\n",
      " [-0.76580414  1.        ]]\n",
      "states [[ 1.          0.23279806]\n",
      " [-0.84980825  1.        ]]\n",
      "states [[-1.         -1.        ]\n",
      " [ 0.39715247  0.95327187]]\n",
      "states [[ 1.         -1.        ]\n",
      " [-0.77530138  1.        ]]\n",
      "states [[-0.48903051 -1.        ]\n",
      " [ 0.05245194 -0.87521197]]\n",
      "states [[ 1.          0.06093768]\n",
      " [-0.07814678 -0.59662374]]\n",
      "states [[-1.          0.76183077]\n",
      " [-0.61753082  0.60338171]]\n",
      "states [[-0.41661962 -0.81322701]\n",
      " [ 0.93354804  0.5248188 ]]\n",
      "states [[-1.          0.07893431]\n",
      " [-0.41409367 -0.78101915]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards\n",
    "        print(\"states\", actions)\n",
    "        # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.          -7.69742537  -1.5\n",
      "   -0.           0.          -7.5934515    5.98822832  -0.\n",
      "    0.         -10.69742775  -1.55886006 -30.          -0.98100001\n",
      "   -7.5934515    5.89012814 -30.          -0.98100001]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.          -6.45862055  -1.5\n",
      "    0.           0.           7.5934515    5.98822832   0.\n",
      "    0.          -7.70090055  -1.55886006 -12.42281055  -0.98100001\n",
      "    7.5934515    5.89012814 -12.42281055  -0.98100001]]\n",
      "[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [-7.697425365447998, -6.458620548248291], [-1.5, -1.5], [-0.0, 0.0], [0.0, 0.0], [-7.593451499938965, 7.593451499938965], [5.9882283210754395, 5.9882283210754395], [-0.0, 0.0], [0.0, 0.0], [-10.697427749633789, -7.700900554656982], [-1.5588600635528564, -1.5588600635528564], [-30.0, -12.422810554504395], [-0.9810000061988831, -0.9810000061988831], [-7.593451499938965, 7.593451499938965], [5.890128135681152, 5.890128135681152], [-30.0, -12.422810554504395], [-0.9810000061988831, -0.9810000061988831]]\n",
      "48\n",
      "tensor [tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,  -7.6974,  -1.5000,  -0.0000,   0.0000,  -7.5935,   5.9882,\n",
      "         -0.0000,   0.0000, -10.6974,  -1.5589, -30.0000,  -0.9810,  -7.5935,\n",
      "          5.8901, -30.0000,  -0.9810]), tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,  -6.4586,  -1.5000,   0.0000,   0.0000,   7.5935,   5.9882,\n",
      "          0.0000,   0.0000,  -7.7009,  -1.5589, -12.4228,  -0.9810,   7.5935,\n",
      "          5.8901, -12.4228,  -0.9810])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def transpose_list(mylist):\n",
    "    return list(map(list, zip(*mylist)))\n",
    "\n",
    "def transpose_to_tensorAsitis(input_list):\n",
    "    make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n",
    "    return list(map(make_tensor, input_list))\n",
    "\n",
    "env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "next_states = env_info.vector_observations\n",
    "print(next_states)\n",
    "s = transpose_list(next_states)\n",
    "print(s)\n",
    "p = np.concatenate((next_states[0], next_states[1]))\n",
    "print(np.shape(p)[0])\n",
    "s = transpose_to_tensorAsitis(next_states)\n",
    "print(\"tensor\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device that gonna be used is  cuda\n"
     ]
    }
   ],
   "source": [
    "# main code that contains the neural network setup\n",
    "# policy + critic updates\n",
    "# see ddpg.py for other details in the network\n",
    "\n",
    "from ddpg import DDPGAgent\n",
    "import torch\n",
    "from utilities import soft_update, transpose_to_tensor, transpose_list , transpose_to_tensorAsitis , giveCurrentAgentsAction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(\"the device that gonna be used is \", device)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class MADDPG:\n",
    "    def __init__(self, state_size , action_size , discount_factor=0.95, tau=0.05 , lr_actor = 2e-4 , lr_critic = 2e-3 , num_agents =2):\n",
    "        super(MADDPG, self).__init__()\n",
    "        \n",
    "        hidden_in_dim = 256\n",
    "        hidden_out_dim = 128\n",
    "        out_ext = 64\n",
    "        # critic input = obs_full + actions = 48+2+2=52\n",
    "        # have to change the agent neurons for sure\n",
    "        \n",
    "        # the no of agents is two because there are only two players\n",
    "        self.maddpg_agent = [DDPGAgent(state_size , action_size,hidden_in_dim, hidden_out_dim, extrem_out = out_ext, num_agents =num_agents , lr_actor = lr_actor, lr_critic = lr_critic), \n",
    "                             DDPGAgent(state_size , action_size, hidden_in_dim, hidden_out_dim, extrem_out = out_ext,num_agents =num_agents, lr_actor = lr_actor, lr_critic = lr_critic)]\n",
    "        \n",
    "        self.num_agents = num_agents \n",
    "        self.action_vector = 2\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.iter = 0\n",
    "\n",
    "    def get_actors(self):\n",
    "        \"\"\"get target_actors of all \"\"\"\n",
    "        \"\"\"get actors of all the agents in the MADDPG object\"\"\"\n",
    "        actors = [ddpg_agent.actor for ddpg_agent in self.maddpg_agent]\n",
    "        return actors\n",
    "\n",
    "    def get_target_actors(self):\n",
    "        \"\"\"the agents in the MADDPG object\"\"\"\n",
    "        target_actors = [ddpg_agent.target_actor for ddpg_agent in self.maddpg_agent]\n",
    "        return target_actors\n",
    "\n",
    "    def act(self, obs_all_agents, noise=0.0 , batch = True):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"\n",
    "        #print(obs_all_agents)\n",
    "        #shape_vec = [np.shape(obs) for obs in obs_all_agents]\n",
    "        #print(\"shape\",shape_vec)\n",
    "        actions = [agent.act(obs, noise , batch = batch) for agent, obs in zip(self.maddpg_agent, obs_all_agents)]\n",
    "        #print(actions)\n",
    "        return actions\n",
    "\n",
    "    def target_act(self,agent_no, obs_all_agents, noise=0.0 , batch = True):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        target = []\n",
    "        for i, obs in enumerate(obs_all_agents):\n",
    "                target_actions = [self.maddpg_agent[i].target_act(obs[i,:], batch = batch) for i in range(agent_no)]\n",
    "                target_actions = torch.stack(target_actions)\n",
    "                target.append(target_actions)\n",
    "        return target\n",
    "    \n",
    "    def target_act_batch(self,agent_no, obs_all_agents, noise=0.0 , batch = True):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        target_actions = [self.maddpg_agent[i].target_act(obs_all_agents[:,i,:] , batch = batch) for i in range(agent_no)]\n",
    "        target_actions = torch.stack(target_actions)\n",
    "        return target_actions\n",
    "    \n",
    "    def act_with_agent(self,agent_no, agent_id ,obs_all_agents, noise=0.0 , batch = True):\n",
    "        \"\"\"get target network actions from all the agents in the MADDPG object \"\"\"\n",
    "        \n",
    "        actions = [self.maddpg_agent[i].actor(obs_all_agents[:,i,:] ,batch = batch) if i == agent_id \\\n",
    "           else self.maddpg_agent[i].actor(obs_all_agents[:,i,:] , batch = batch).detach() for i in range(agent_no)]\n",
    "        \n",
    "        actions = torch.stack(actions)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def update(self, samples, agent_number , update_actor = True):\n",
    "        \"\"\"update the critics and actors of all the agents \"\"\"\n",
    "\n",
    "        # need to transpose each element of the samples\n",
    "        # to flip obs[parallel_agent][agent_number] to\n",
    "        # obs[agent_number][parallel_agent]\n",
    "        \n",
    "        \n",
    "        \n",
    "        obs, obs_full, action, reward, next_obs, next_obs_full, done = samples\n",
    "        samples = (np.array(obs), obs_full, action, reward, np.array(next_obs), next_obs_full, done)\n",
    "        \n",
    "        batch_size = np.shape(obs_full)[0]\n",
    "        Batch_use = True if batch_size >1 else False\n",
    "        \n",
    "        action_size = self.num_agents * self.action_vector\n",
    "        \n",
    "        obs, obs_full, action, reward, next_obs, next_obs_full, done = map(transpose_to_tensorAsitis, samples)\n",
    "        \n",
    "        \n",
    "        obs_full = torch.stack(obs_full).to(device)\n",
    "        next_obs_full = torch.stack(next_obs_full).to(device)\n",
    "        \n",
    "        agent = self.maddpg_agent[agent_number]\n",
    "        agent.critic_optimizer.zero_grad()\n",
    "\n",
    "        # critic loss = batch mean of (y- Q(s,a) from target network)^2\n",
    "        #y = reward of this timestep + discount * Q(st+1,at+1) from target network\n",
    "        # !crictal logic error have to change her to the agents observation only\n",
    "        \n",
    "        \n",
    "        next_obs = torch.stack(next_obs).to(device)\n",
    "        obs = torch.stack(obs).to(device)\n",
    "        \n",
    "        action = torch.stack(action).to(device)\n",
    "        #print(action[:,agent_number,:] , action)\n",
    "        #target_actions = self.target_act_batch(2,next_obs)\n",
    "        \n",
    "        actions_next = agent.target_act(next_obs[:,agent_number,:], batch = Batch_use).to(device)\n",
    "        \n",
    "        #print(actions_next.size())\n",
    "        \n",
    "        if agent_number == 0:\n",
    "            target_actions = torch.cat((actions_next, action[:,num_agents -1 - agent_number,:]), dim=1)\n",
    "        else:\n",
    "            target_actions = torch.cat((action[:,num_agents - agent_number -1,:], actions_next), dim=1)\n",
    "        \n",
    "        target_actions = target_actions.to(device)\n",
    "        #batch size and action size\n",
    "        #target_critic_input = torch.cat((next_obs_full.view(-1,batch_size).t(),target_actions.view(-1,action_size)), dim=1).to(device)\n",
    "        \n",
    "        #Current_Agent_actions_target,other_agent_Action_target = giveCurrentAgentsAction(target_actions , agent_number , Tuples = False, batch = Batch_use)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "                #critic_state = torch.cat((next_obs_full.view(-1,batch_size).t(),target_actions) , dim=1)\n",
    "                q_next=agent.target_critic.critic_forward(next_obs_full.view(-1,batch_size).t() , target_actions.view(-1,action_size)).to(device)\n",
    "        \n",
    "        #indices = torch.tensor([1])\n",
    "                                                        \n",
    "                                                        \n",
    "                                                        \n",
    "                                                        \n",
    "        reward = torch.stack(reward).to(device)\n",
    "                                                                                              \n",
    "        done = torch.stack(done).to(device)\n",
    "        \n",
    "        y = reward[:,agent_number].view(-1, 1) + self.discount_factor * q_next * (1 - done[:,agent_number].view(-1, 1)).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Current_Agent_actions,other_agent_Action = giveCurrentAgentsAction(action , agent_number , batch = Batch_use)\n",
    "        #print(action,Current_Agent_actions.size(),other_agent_Action.size())\n",
    "        #critic_input = torch.cat((obs_full.view(-1,batch_size).t(), other_agent_Action), dim=1).to(device)\n",
    "        \n",
    "        q = agent.critic.critic_forward(obs_full.view(-1,batch_size).t(),  action.view(-1,action_size)).to(device)\n",
    "\n",
    "        huber_loss = torch.nn.SmoothL1Loss()\n",
    "        mse_loss = F.mse_loss\n",
    "        critic_loss = huber_loss(q, y.detach())\n",
    "        #print(critic_loss)\n",
    "        critic_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), 0.3)\n",
    "        agent.critic_optimizer.step()\n",
    "\n",
    "        #update actor network using policy gradient\n",
    "        if(update_actor):\n",
    "            agent.actor_optimizer.zero_grad()\n",
    "\n",
    "            # make input to agent\n",
    "            # detach the other agents to save computation\n",
    "            # saves some time for computing derivative\n",
    "\n",
    "\n",
    "    #         q_input = [ self.maddpg_agent[i].actor(ob) if i == agent_number \\\n",
    "    #                    else self.maddpg_agent[i].actor(ob).detach()\n",
    "    #                    for i, ob in enumerate(obs) ]\n",
    "\n",
    "            q_input = agent.actor(obs[:,agent_number,:] ,batch = Batch_use)\n",
    "        \n",
    "        \n",
    "            if agent_number == 0:\n",
    "                q_input = torch.cat((q_input, action[:,num_agents - agent_number -1,:]), dim=1)\n",
    "            else:\n",
    "                q_input = torch.cat((action[:,num_agents - agent_number -1,:], q_input), dim=1)\n",
    "\n",
    "            q_input = q_input.to(device)\n",
    "            \n",
    "            #Current_Agent_Qin,other_agent_Qin = giveCurrentAgentsAction(q_input , agent_number , batch = Batch_use , Tuples = False)\n",
    "\n",
    "            # combine all the actions and observations for input to critic\n",
    "            # many of the obs are redundant, and obs[1] contains all useful information already\n",
    "\n",
    "            #q_input2 = torch.cat((obs_full.view(-1,batch_size).t(), other_agent_Qin), dim=1).to(device)\n",
    "\n",
    "            # get the policy gradient\n",
    "            actor_loss = -agent.critic.critic_forward(obs_full.view(-1,batch_size).t(), q_input.view(-1,action_size)).mean()\n",
    "            \n",
    "            actor_loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),0.3)\n",
    "            agent.actor_optimizer.step()\n",
    "\n",
    "    def update_targets(self):\n",
    "        \"\"\"soft update targets\"\"\"\n",
    "        self.iter += 1\n",
    "        for ddpg_agent in self.maddpg_agent:\n",
    "            soft_update(ddpg_agent.target_actor, ddpg_agent.actor, self.tau)\n",
    "            soft_update(ddpg_agent.target_critic, ddpg_agent.critic, self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 0/4000   0% ETA:  --:--:-- |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic Network(\n",
      "  (bnS): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=48, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ") Network(\n",
      "  (bnS): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=48, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ") optim Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "actor Network(\n",
      "  (bnS): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      ") Network(\n",
      "  (bnS): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      ") optim Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "critic Network(\n",
      "  (bnS): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=48, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ") Network(\n",
      "  (bnS): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=48, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
      ") optim Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "actor Network(\n",
      "  (bnS): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      ") Network(\n",
      "  (bnS): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
      ") optim Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards 0.0 0\n",
      "agent1/mean_episode_rewards -0.009999999776482582 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 100/4000   2% ETA:  0:33:43 |                                       | \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 102/4000   2% ETA:  0:33:44 |                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0030999998562037945 100\n",
      "agent1/mean_episode_rewards -0.0008999998308718204 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 199/4000   4% ETA:  0:33:03 ||                                      | \r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAACTCAYAAAC3ft8fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAG0NJREFUeJzt3Xl0nHd97/H3V5Il79Y6imNb3iRrlABOYiVOsB1bo4QsnJJCUxIIJIctTQkkcLmFsNxCob0n5dDSey+XtilLQtsb6AVOm5Y1lWRnX+xshHgk77ESR6PNm2RZ0sz3/vE8FsJXlgZbo9HyeZ2j45lnfs8zv59lz3d+z+95vl9zd0RERDIlJ9sdEBGR6U2BRkREMkqBRkREMkqBRkREMkqBRkREMkqBRkREMkqBRkREMkqBRkREMkqBRkREMiov2x2YDEpLS33FihXZ7oaIyJSyY8eODncvG6tdVgONmV0L/A8gF/iWu9972usFwPeAdUAncJO77w9f+yzwISAJ3OXuv0jnmCNZsWIF27dvH69hiYjMCGZ2IJ12WTt1Zma5wP8GrgMuAN5jZhec1uxDQLe7VwJfB/4y3PcC4GbgQuBa4JtmlpvmMUVEZAJlc43mMmC3u+91937g+8ANp7W5AXggfPxDoN7MLNz+fXc/6e77gN3h8dI5pojIjHf85CA/f/kQx/oGMv5e2Tx1tgQ4OOx5K7D+TG3cfdDMjgAl4fanTtt3Sfh4rGMCYGa3A7cDVFRUnN0IRESmkP0dPTTGEzTGEzy9r5OBpPPNWy7h+jcvzuj7ZjPQ2AjbTq9ZcKY2Z9o+0gxtxDoI7n4fcB9AbW2taiWIyLQzkEzx7P4uGncmaGxOsLe9B4DVZfP4wIaVxKIR1i0vyng/shloWoFlw54vBV4/Q5tWM8sDFgFdY+w71jFFRKatzuMn2drcTmM8wSMt7Rw7OUh+bg7rVxXz/suXE4tGWF4yb0L7lM1A8yxQZWYrgdcIFvffe1qbh4DbgCeBG4FGd3czewj4P2b218D5QBXwDMFMZ6xjiohMG+7OK4eODs1aXjh4GHcoW1DA9W9eTF00wsaqUuYXZO/jPmvvHK65fAz4BcGlyN9x91+b2ZeB7e7+EPBt4B/NbDfBTObmcN9fm9m/AK8Ag8Cd7p4EGOmYEz02EZFM6u0f5IndnTTEEzTFE7xxtA+AtUsXcXd9FfXRci48fyE5OSOtMkw8UynnYI1G99GIyGR2sKuXpuZgIf+JPZ30D6aYl5/LpqoyYjURtlSXEVkwe0L7ZGY73L12rHbKDCAiMgkNJlM8f/AwDTsTNMbbaGk7DsDykrncsr6C+mg5l64soiAvN8s9HZsCjYjIJHG4t59tLcFC/tbmdo6cGCAvx7h0RTGfv34ZsZoIq0rnEdxOOHUo0IiIZIm7sytxnIadwVrL9gNdpByK5+VTXxOhPlrOpjWlLJw9K9tdPScKNCIiE6hvIMlTeztpjCdo2JngtcMnALhg8ULurKukLhph7dJCcifJQv54UKAREcmwN470Dd2R//juDk4MJJk9K4eNlaVhcClj8aI52e5mxijQiIiMs2TKebH1ME3hrOWVQ0cBWFI4hxvXLSVWE+GKVSXMnjX5F/LHgwKNiMg4ONo3wKMtHeFCfoLOnn5yDNYtL+Iz10aJRSOsKZ8/5Rbyx4MCjYjIWdrbfnxoreXZ/V0MppxFc2axpbqMWDTC5jVlFM7Nz3Y3s06BRkQkTf2DKZ7Z1xWut7Sxv7MXgDXl8/nwplXEohEuqSgkLzebFVgmHwUaEZFRtB87GdyRvzPBY7s7OH5ykPy8HN66uoQPblxJXXWEZcVzs93NSU2BRkRkmFTK+fXrR4dmLS+2HgGgfGEBv7f2fGLRCBsqS5ibr4/PdOlvSkRmvJ6Tgzy2u4PGnQmamhMkjp3EDC5aVsinrl5DrCbCBYsXzsiF/PGgQCMiM9Krnb00xttoiCd4em8X/ckUCwryuHJNuJBfXUbp/IJsd3NaUKARkRlhIJlix4HuoRsndyeCJJWryuZx6xXLidVEuHRFMbO0kD/uFGhEZNrq6ulnW0tw+fEjLe0c7RtkVq6xfmUJ77msglg0wsrSia02ORMp0IjItOHuxN84NjRref7VblIOpfPzuebC86ivibChspQFUzxJ5VSjQCMiU1rfQJIn9nQMZUB+/UhQbfLNSxbxsVgV9dEIb16yaNJUm5yJFGhEZMp57fAJGsMyxo/v7uDkYIq5+blsrCzl7quqqKuOEFk4sdUm5cwUaERk0kumnBcOdofVJhPE3zgGwLLiOUNrLetXFU+JapMzkQKNiExKR3oH2LarnaYwSWV37wC5OUbt8iI+d32QpHJ12cxMUjnVKNCIyKTg7uxpPz40a9l+oJtkyimaO4st1RFi0QhXVpWxaK4W8qcaBRoRyZqTg0me3hskqWyIt3GwK6g2GT1vAXdsDpJUXrSsaFpVm5yJFGhEZEK1He2jKbz8+LHdHfT2JynIy2FDZSl/dOVq6qIRlhRO32qTM5ECjYhkVCrlvPTakaEklS+/FlSbPH/RbN51yRJi0QhXrCplTr4W8qcrBRoRGXfH+gZ4bFdQbbKpuZ2O4yfJMbi4oog/uaaa+poI1eULtJA/QyjQiMi42N/RQ0M4a3lmXxcDSWfh7Dw2V0eIRcvYvCZC8TxVm5yJFGhE5Kz0D6bYvr9rKN3L3o4eACoj8/nghpXURSOsW16kJJWiQCMi6es4fpKtzcG9LY+0tHPs5CD5uTlcvrokyIAcLaeiRNUm5belHWjMbCNQ5e7fNbMyYL6778tc10Qk29yDapNN8QQN8QQvth7GHSILCnj7WxaH1SZLmVeg76xyZmn96zCzLwK1QDXwXWAW8E/Ahsx1TUSyobd/kMd3d9IYb6Mp3s4bR4MklWuXFfKJ+jXUh9UmlaRS0pXu15B3AhcDzwG4++tmtiBjvRKRCXWwq5em5qBuy5N7O+kfTDG/II9NVaXEohG2VEcoW6Bqk3J20g00/e7uZuYAZqZKQSJT2GAyxXOvHqYh3kZTPEFLW1BtckXJXN63fjn1YbXJ/Dwt5Mu5SzfQ/IuZ/T1QaGYfAT4I/EPmuiUi4+1wbz/bWtpp2JlgW0s7R04MkJdjXLaymHfXLiMWjbCqbH62uynTUFqBxt2/ZmZXA0cJ1mn+1N0fzmjPROScuDstbceH7sjfcSCoNlkyL5+rasqpr4mwsaqUhao2KRk2ZqAxs1zgF+5+FaDgIjKJ9Q0keXJvJ41hBuTXDgdJKi88fyF31lUSi0ZYu7RQC/kyocYMNO6eNLNeM1vk7kcmolMikr5DR07QFG+nMd7GY7s76BtIMWdWLhsqS/lYrJK66gjnLVK1ScmedNdo+oBfmdnDQM+pje5+19m8qZkVAz8AVgD7gXe7e/cI7W4DvhA+/XN3fyDcvg64H5gD/BS4O7xY4UvAR4D2cJ/PuftPz6aPIpNVMuW82Hp4aNbyyqEgSeXSojncVLuMumiEy1eVMHuWklTK5JBuoPlJ+DNe7gEa3P1eM7snfP6Z4Q3CYHTq/h0HdpjZQ2FA+lvgduApgkBzLfCzcNevu/vXxrGvIll3tG+AR1s6aIi3sbW5na6efnJzjHUVRdxzXVBtsiqiapMyOaV7McADZpYPrAk3Nbv7wDm87w3AlvDxA8BWTgs0wDXAw+7eBRDOpq41s63AQnd/Mtz+PeD3+U2gEZny3J29HT1Ds5Zn93cxmHIK585iy5oy6qIRNq8po3CuklTK5JduZoAtBAFhP2DAMjO7zd0fOcv3LXf3QwDufsjMIiO0WQIcHPa8Ndy2JHx8+vZTPmZmtwLbgU+NdEoOwMxuJ5gVUVFRcZbDEBk//YMpntnXRUO8jcZ4ggOdvQBUly/gI1cG1SYvXlZInpJUyhST7qmzvwLe5u7NAGa2BngQWHemHczsP4HzRnjp82m+50jnAHyU7RCcUvtK+PwrYb8/ONLB3f0+4D6A2tpaH6mNSKYljvWxNd5OYzzBo7va6elPkp+Xw4bVJXx4Y5ABeWmRklTK1JZuoJl1KsgAuHuLmY168X14OfSIzKzNzBaHs5nFQGKEZq385vQawFKCU2yt4ePh218P37Nt2Hv8A/Afo/VRZKKlUkGSylN35L/YGlzIed7C2dxw8RLqoxHeulrVJmV6STfQbDezbwP/GD6/BdhxDu/7EHAbcG/457+N0OYXwH83s6Lw+duAz7p7l5kdM7PLgaeBW4H/BXAqeIXt3wm8fA59FBkXx08O8tiuDpriCRqbE7QfO4kZXLyskP/6tjXEouXULFa1SZm+0g00fwzcCdxFcOrqEeCb5/C+9xKktfkQ8CrwhwBmVgvc4e4fDgPKV4Bnw32+fOrCgLA/9xNc3vwzfnMhwFfN7CKCU2f7gT86hz6KnLUDnT1DBcGe3ttFfzLFgoI8rqwuI1YdYUt1GSXzlaRSZgZzH3t5Ikyi2efuyfB5LlDg7r0Z7t+EqK2t9e3bt2e7GzKFDSRTbN/fHWZAbmNPe3C72aqyedRHI8Si5dSuULVJmV7MbIe7147VLt0ZTQNwFXA8fD4H+CXw1rPrnsjU19XTz9bmYNayraWdY32DzMo1Ll9Vwi3rlxOLRlhRqkTnIukGmtnufirI4O7HzUyXwsiM4u7E3zhGYzyYtTx/MKg2WbaggOvedB6xaDkbq0qZr2qTIr8l3f8RPWZ2ibs/B0NrKScy1y2RyeFEf5In9nQMrbccOhJUm3zL0kXcFauivibCm85fpCSVIqNIN9B8Avi/ZvY6wUL7+cBNGeuVSBa9dvhEEFh2tvHEnk5ODqaYm5/LpqpSPnnVGrZUlxFZqCSVIukaNdCY2aXAQXd/1syiBFdxvQv4ObBvAvonknHJlPP8q91Ds5b4G8cAqCiey3suq6C+JsJlK4spyNO9LSJnY6wZzd8TXAQAcAXwOeDjwEUEd9XfmLmuiWTOkd4Btu1qp3FnG9ta2unuDapN1q4o4vPX11AXjbC6bJ7ubREZB2MFmtxh967cBNzn7j8CfmRmL2S2ayLjx93ZnQiqTTbEE+w40E0y5RTPy6euOkKsJsKmqjIWzVG1SZHxNmagMbM8dx8E6gmTUKa5r0hW9Q0keXpfF40722hsTnCwK7h+pWbxQv5482rqohEuWlZIrhbyRTJqrGDxILDNzDoIrjJ7FMDMKgFV25RJp+1oH03hrOXx3R309ieZPSuHDatLuWPzauqqI5xfOCfb3RSZUUYNNO7+F2bWACwGfum/SSOQQ7BWI5JVqZTz0mtHhmYtL78WVJtcUjiHP7hkKbFohCtWq9qkSDaNefrL3Z8aYVtLZrojMrZjfQM8tquDhniCrc0JOo73k2OwbnkRn762mvpoOWvKVW1SZLLQOotMCfs6emjY2UZTc4Jn9nUxkHQWzs5jS3WEWFhtsmieqk2KTEYKNDIp9Q+m2L6/i4Z4gqZ4gr0dQZLKqsh8PrhxJfXRci6pULVJkalAgUYmjY7jJ9na3E5jvI1HWjo4fnKQ/Nwcrlhdwm1vXUEsGmFZsVLsiUw1CjSSNe5BtclTd+S/2BokqSxfWMDvrV1MLFrOhsoS5ubrn6nIVKb/wTKhevvDapNhev22o0G1ybVLC/nkVWuIRSNceP5CLeSLTCMKNJJxB7t6h2YtT+7tpH8wxfyCPK5cU0osWs6W6jJKVW1SZNpSoJFxN5hMseNAN43NCRp3JtiVCEoZrSqdx/svX059NELtimLy87SQLzITKNDIuOju6WdbS/tQtckjJ4IkletXFXPzZRXEohFWqtqkyIykQCNnxd1paTtOQ7yNxp0Jnnu1m5RD6fx8rr6gnPpohI1VpSyYrSSVIjOdAo2krW8gyZN7OofWW147HCSpfNOShXwsVkUsGuEtS1RtUkR+mwKNjOrQkVPVJhM8vqeDvoGg2uSGylI+HqukLhqhXNUmRWQUCjTyW5Ip54WDh4cyIO88FCSpXFY8h5svraAuGmH9ymIlqRSRtCnQCEdODPDornYadybY2tJOV08/uTnGuuVFfPa6KLFohMqIklSKyNlRoJmB3J097T3hrKWN7fu7GUw5hXNnUVcdoS4aYXNVGYvmaiFfRM6dAs0McXIwyTP7umjYmaCpOcGBzl4Aouct4PYrV1FfE+GiZUWqNiki406BZhpLHOtja7ydhngbj+3qoKc/SUFeDhsqS/nwplXEohGWqNqkiGSYAs00kko5L79+ZOjy45dag2rbixfN5vcvXkIsGuGtq0uZk6+FfBGZOAo0U9zxk0GSysZ4G03N7bQfC5JUXlJRxJ9cU00sGiF63gIt5ItI1ijQTEEHOnuG1lqe2tvJQNJZMDuPzWvKqK+JsHlNhGJVmxSRSUKBZgoYSKbYvr+bxngbjfEEe9qDapOVkfl8YMNKYtEI65YXMUvVJkVkElKgmaS6evrZ2hzcNPlISzvH+oJqk+tXFfO+y5cTi0ZYXqIklSIy+SnQTBLuzs5Dx4ZmLc8fDKpNli0o4Po3LSZWE2FjZSnzCvQrE5GpRZ9aWXSiP8kTezpoiCdoiic4dKQPgLVLF3F3fRX10XIuPH+hklSKyJSmQDPBWrt7aQovP35iTycnB1PMy89lU1UZn7w6wpbqMiILlKRSRKYPBZoMS6ac51/tpiHMgNzcdgyA5SVzee/6Cuqj5Vy6soiCPN3bIiLTU1YCjZkVAz8AVgD7gXe7e/cI7W4DvhA+/XN3fyDc/hfArUCRu88f1r4A+B6wDugEbnL3/RkbyBkc6R1ga0twOmxrSzuHe4Nqk5euKOYLb6+hLhphVek83dsiIjNCtmY09wAN7n6vmd0TPv/M8AZhMPoiUAs4sMPMHgoD0r8D3wB2nXbcDwHd7l5pZjcDfwnclNmhBAv5uxPHh2YtO17tJplyiuflE4tGqI+Ws2lNKQtVbVJEZqBsBZobgC3h4weArZwWaIBrgIfdvQvAzB4GrgUedPenwm0jHfdL4eMfAt8wM3N3H9/uB57d38V/vPg6DfEErd1BtckLFi/ko1tWUxeNsHZpoZJUisiMl61AU+7uhwDc/ZCZRUZoswQ4OOx5a7htNEP7uPugmR0BSoCO0xua2e3A7QAVFRW/8wAAfvarN/jB9oNsrCzjo1sqqYuWsXiRklSKiAyXsUBjZv8JnDfCS59P9xAjbBtrZpL2Pu5+H3AfQG1t7VnNeD4eq+TT11ar2qSIyCgyFmjc/aozvWZmbWa2OJzNLAYSIzRr5Ten1wCWEpxiG00rsAxoNbM8YBHQ9bv0+3dRpHxiIiJjylZyrIeA28LHtwH/NkKbXwBvM7MiMysC3hZuS/e4NwKNmVqfERGR9GQr0NwLXG1mu4Crw+eYWa2ZfQsgvAjgK8Cz4c+Xh10Y8FUzawXmmlmrmX0pPO63gRIz2w38F4Kr2UREJItMX/jBzNqBA2e5eykjXGwwzWnMM4PGPDOcy5iXu3vZWI0UaM6RmW1399ps92Miacwzg8Y8M0zEmFXAREREMkqBRkREMkqB5tzdl+0OZIHGPDNozDNDxsesNRoREckozWhERCSjFGhERCSjFGjSYGbfMbOEmb18htfNzP6nme02s5fM7JKJ7uN4S2PMt4RjfcnMnjCztRPdx/E21piHtbvUzJJmduNE9S1T0hmzmW0xsxfM7Ndmtm0i+5cJafzbXmRm/25mL4Zj/sBE93E8mdkyM2sys53heO4eoU1GP8MUaNJzP0GJgjO5DqgKf24H/nYC+pRp9zP6mPcBm939LQQZHKbDIur9jD5mzCyXoM7RWOmQpor7GWXMZlYIfBN4h7tfCPzhBPUrk+5n9N/zncAr7r6WIN/iX5nZVE5sOAh8yt1rgMuBO83sgtPaZPQzTIEmDe7+CKMn57wB+J4HngIKw2ShU9ZYY3b3J4ZVRX2KIOnplJbG7xng48CPGDkR7JSTxpjfC/zY3V8N20/5cacxZgcWWFDwan7YdnAi+pYJ7n7I3Z8LHx8DdvL/l1zJ6GeYAs34OJvaOdPJh4CfZbsTmWZmS4B3An+X7b5MoDVAkZltNbMdZnZrtjs0Ab4B1ACvA78C7nb3VHa7ND7MbAVwMfD0aS9l9DMsW4XPppuzqZ0zLZhZHUGg2ZjtvkyAvwE+4+7JEaq7Tld5wDqgHpgDPGlmT7l7S3a7lVHXAC8AMWA18LCZPeruR7PbrXNjZvMJZuOfGGEsGf0MU6AZH6fq4JyylODb0LRmZm8BvgVc5+6d2e7PBKgFvh8GmVLgejMbdPd/zW63MqoV6HD3HqDHzB4B1gLTOdB8ALg3LDGy28z2AVHgmex26+yZ2SyCIPPP7v7jEZpk9DNMp87Gx0PAreGVG5cDR06Vqp6uzKwC+DHw/mn+7XaIu6909xXuvgL4IfDRaR5kIKgVtcnM8sxsLrCe4Bz/dPYqwQwOMysHqoG9We3ROQjXmr4N7HT3vz5Ds4x+hmlGkwYze5Dg6pPSsA7OF4FZAO7+d8BPgeuB3UAvwTeiKS2NMf8pUAJ8M/yGPzjVs96mMeZpZ6wxu/tOM/s58BKQAr7l7qNe/j3ZpfF7/gpwv5n9iuCU0mfcfSqXDtgAvB/4lZm9EG77HFABE/MZphQ0IiKSUTp1JiIiGaVAIyIiGaVAIyIiGaVAIyIiGaVAIyIiGaVAI3IOwizOLwz7uWeM9neMRxoXM9tvZqVnsd81ZvYlMysys5+eaz9E0qH7aETOzQl3vyjdxpPgfpxNQBNwJfB4lvsiM4QCjUgGmNl+4AdAXbjpve6+28y+BBx396+Z2V3AHQSZgV9x95vNrBj4DrCK4Ma52939JTMrAR4EyghSodiw93ofcBeQT5As8aPunjytPzcBnw2PewNQDhw1s/Xu/o5M/B2InKJTZyLnZs5pp85uGvbaUXe/jCAb8N+MsO89wMVhTZ87wm1/Bjwfbvsc8L1w+xeBx9z9YoJ0IRUAZlYD3ARsCGdWSeCW09/I3X8AXAK87O5vBl4O31tBRjJOMxqRczPaqbMHh/359RFefwn4ZzP7V+BUzrSNwB8AuHujmZWY2SKCU13vCrf/xMxO1QKqJ8iu/GyYCmgOZ66VUwXsCR/PDWuTiGScAo1I5vgZHp/ydoIA8g7gv5nZhYyern2kYxjwgLt/drSOmNl2gozTeWb2CrA4zHv1cXd/dPRhiJwbnToTyZybhv355PAXzCwHWObuTcCngUKCao6PEJ76MrMtBCn6j562/TqgKDxUA3CjmUXC14rNbPnpHQkTnv6EYH3mq8Dn3f0iBRmZCJrRiJybOcMy4gL83N1PXeJcYGZPE3yhe89p++UC/xSeFjPg6+5+OLxY4Ltm9hLBxQC3he3/DHjQzJ4DthGkssfdXzGzLwC/DIPXAEHN+wMj9PUSgosGPgqcKV28yLhT9maRDAivOqud4unlRcaFTp2JiEhGaUYjIiIZpRmNiIhklAKNiIhklAKNiIhklAKNiIhklAKNiIhk1P8D+VQ26CQNZ5UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 201/4000   5% ETA:  0:33:05 |/                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 200\n",
      "agent1/mean_episode_rewards -0.004599999897181988 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 214/4000   5% ETA:  0:32:49 |\\\\                                     | \r"
     ]
    }
   ],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "#import envs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from buffer import ReplayBuffer\n",
    "#from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "#from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "\n",
    "# keep training awake\n",
    "from workspace_utils import keep_awake\n",
    "\n",
    "# for saving gif\n",
    "import imageio\n",
    "\n",
    "def seeding(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def pre_process(entity, batchsize):\n",
    "    processed_entity = []\n",
    "    for j in range(3):\n",
    "        list = []\n",
    "        for i in range(batchsize):\n",
    "            b = entity[i][j]\n",
    "            list.append(b)\n",
    "        c = torch.Tensor(list)\n",
    "        processed_entity.append(c)\n",
    "    return processed_entity\n",
    "\n",
    "buffer = ReplayBuffer(int(1e4))\n",
    "def main():\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "\n",
    "    # examine the state space \n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    \n",
    "    seeding()\n",
    "    # number of parallel agents\n",
    "    #parallel_envs = num_agents\n",
    "    # number of training episodes.\n",
    "    # change this to higher number to experiment. say 30000.\n",
    "    \n",
    "    number_of_episodes = 4000\n",
    "    update_actor_after = 1\n",
    "    update_actor_every = 1\n",
    "    episode_length = 100\n",
    "    batchsize = 128\n",
    "    # how many episodes to save policy and gif\n",
    "    save_interval = 1000\n",
    "    t = 0\n",
    "    \n",
    "    LR_ACTOR = 1e-4\n",
    "    LR_CRITIC = 1e-3\n",
    "    \n",
    "    # amplitude of OU noise\n",
    "    # this slowly decreases to 0\n",
    "    noise = 5.0\n",
    "    noise_reduction = 0.999\n",
    "\n",
    "    # how many episodes before update\n",
    "    episode_per_update =  2\n",
    "    no_of_updates_perTime = 1\n",
    "\n",
    "    log_path = os.getcwd()+\"/log\"\n",
    "    model_dir= os.getcwd()+\"/model_dir\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    #torch.set_num_threads(parallel_envs)\n",
    "    #env = envs.make_parallel_env(parallel_envs)\n",
    "    \n",
    "    # keep 5000 episodes worth of replay\n",
    "    #buffer = ReplayBuffer(int(1000*episode_length))\n",
    "    \n",
    "    # initialize policy and critic\n",
    "    maddpg = MADDPG(state_size , action_size,num_agents = num_agents , lr_actor = LR_ACTOR , lr_critic = LR_CRITIC)\n",
    "    #logger = SummaryWriter(log_dir=log_path)\n",
    "    agent0_reward = []\n",
    "    agent1_reward = []\n",
    "    #agent2_reward = []\n",
    "\n",
    "    # training loop\n",
    "    # show progressbar\n",
    "    import progressbar as pb\n",
    "    widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "              pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "    \n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "\n",
    "    # use keep_awake to keep workspace from disconnecting\n",
    "    for episode in range(0, number_of_episodes):\n",
    "\n",
    "        timer.update(episode)\n",
    "\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        reward_this_episode = np.zeros((1, num_agents))\n",
    "        \n",
    "        #all_obs = env.reset() #\n",
    "        obs = states\n",
    "        obs_full = np.concatenate((states[0], states[1]))\n",
    "\n",
    "        #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "        # save info or not\n",
    "        save_info = ((episode) % save_interval < 1 or episode==number_of_episodes- 1)\n",
    "        tmax = 0\n",
    "        \n",
    "        #resetting noise \n",
    "        for i in range(num_agents):\n",
    "            maddpg.maddpg_agent[i].noise.reset()\n",
    "            \n",
    "        for episode_t in range(episode_length):\n",
    "\n",
    "            t += 1\n",
    "            \n",
    "            update_act = True if (episode > update_actor_after or episode % update_actor_every == 0 ) else False\n",
    "            # explore = only explore for a certain number of episodes\n",
    "            # action input needs to be transposed\n",
    "            actions = maddpg.act(transpose_to_tensorAsitis(obs), noise=noise, batch = False)\n",
    "            noise *= noise_reduction\n",
    "            \n",
    "            actions_for_env = torch.stack(actions).cpu().detach().numpy()\n",
    "            \n",
    "            # transpose the list of list\n",
    "            # flip the first two indices\n",
    "            # input to step requires the first index to correspond to number of parallel agents\n",
    "            #actions_for_env = np.rollaxis(actions_array,1)\n",
    "            \n",
    "            # step forward one frame\n",
    "            #print(\"actions i got \", actions_array,\"actions that i am doing\" ,actions_for_env)\n",
    "            \n",
    "            env_info = env.step(actions_for_env)[brain_name]\n",
    "            \n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards\n",
    "            \n",
    "            rewards_for_env = np.hstack(rewards)\n",
    "\n",
    "            \n",
    "            obs = states\n",
    "            obs_full = np.concatenate((states[0], states[1]))\n",
    "            next_obs = next_states\n",
    "            next_obs_full = np.concatenate((next_states[0], next_states[1]))\n",
    "            # add data to buffer\n",
    "            transition = (np.array([obs]), np.array([obs_full]), np.array([actions_for_env]), np.array([rewards_for_env]), np.array([next_obs]), np.array([next_obs_full]), np.array([dones] , dtype = 'float'))\n",
    "            buffer.push(transition)\n",
    "            \n",
    "            reward_this_episode += rewards\n",
    "\n",
    "            obs, obs_full = next_obs, next_obs_full\n",
    "            \n",
    "            # update once after every episode_per_update\n",
    "            if len(buffer) > batchsize and episode % episode_per_update == 0:\n",
    "                for _ in range(no_of_updates_perTime):\n",
    "                    for a_i in range(num_agents):\n",
    "                        samples = buffer.sample(batchsize)\n",
    "                        #updating the weights of the n/w\n",
    "                        maddpg.update(samples, a_i , update_actor = update_act)\n",
    "                    maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "                \n",
    "            if np.any(dones):\n",
    "                # if the episode is done the loop is break to the next episode\n",
    "                break\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            agent0_reward.append(reward_this_episode[0,0])\n",
    "            agent1_reward.append(reward_this_episode[0,1])\n",
    "            \n",
    "\n",
    "        if episode % 100 == 0 or episode == number_of_episodes-1:\n",
    "            avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward)]\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(212)\n",
    "            plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "#             plt.plot(np.arange(1, len(agent0_reward)+1), agent0_reward)\n",
    "#             plt.plot(np.arange(1, len(agent1_reward)+1), agent1_reward)\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Episode #')\n",
    "            plt.show()\n",
    "            agent0_reward = []\n",
    "            agent1_reward = []\n",
    "            for a_i, avg_rew in enumerate(avg_rewards):\n",
    "                #logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "                print('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "\n",
    "        #saving model\n",
    "        save_dict_list =[]\n",
    "        if save_info:\n",
    "            for i in range(num_agents):\n",
    "\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "\n",
    "                torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "                \n",
    "            # save gif files\n",
    "            #imageio.mimsave(os.path.join(model_dir, 'episode-{}.gif'.format(episode)), \n",
    "                            #frames, duration=.04)\n",
    "    timer.finish()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buffer = ReplayBuffer(int(1000*episode_length))\n",
    "samples = buffer.sample(50)\n",
    "print(np.shape(samples))\n",
    "obs, obs_full, action, reward, next_obs, next_obs_full, done = samples\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3]])\n",
    "b = np.array([[0,4,5]])\n",
    "c = np.concatenate(a+b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1,2,3,4,5,7])\n",
    "b =torch.Tensor([3,2,3,4,5,7])\n",
    "d =torch.Tensor([1,2,3,4,5,7])\n",
    "c = [a,b,d]\n",
    "print(c)\n",
    "c = torch.stack(c)\n",
    "print(c)\n",
    "p = c\n",
    "#p= torch.stack(p)\n",
    "print(p)\n",
    "agent_number = 1\n",
    "\n",
    "d = torch.cat((p[0:agent_number,:], p[agent_number+1:,:]))\n",
    "p[agent_number , :]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
