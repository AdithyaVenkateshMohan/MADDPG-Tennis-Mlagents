{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement tensorflow==1.7.1 (from unityagents==0.4.0) (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 2.0.0a0)\n",
      "No matching distribution found for tensorflow==1.7.1 (from unityagents==0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "path = r\"C:\\Users\\adith\\Desktop\\Meng Robotics\\reinforcement\\Banana\\MADDPG\\Multiagent\\Tennis_Windows_x86_64\\Tennis\"\n",
    "env = UnityEnvironment(file_name=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states [[ 1.          0.72424854]\n",
      " [-0.69863828 -0.3613579 ]]\n",
      "states [[-1.          0.75278837]\n",
      " [-0.08794365 -0.47409759]]\n",
      "states [[-0.4506617   0.29126386]\n",
      " [-1.         -1.        ]]\n",
      "states [[ 1.          0.39542537]\n",
      " [-0.89245731  1.        ]]\n",
      "states [[ 1.         -0.16606194]\n",
      " [-1.         -1.        ]]\n",
      "states [[-0.29749465 -1.        ]\n",
      " [-0.25391974  0.1473507 ]]\n",
      "states [[1.         0.46372589]\n",
      " [0.31253627 0.23265764]]\n",
      "states [[ 0.78016621  0.83620128]\n",
      " [-0.92287497  0.67271402]]\n",
      "states [[-0.49236521 -0.30525434]\n",
      " [-0.56227177  0.0288283 ]]\n",
      "states [[-0.43874554  0.70045872]\n",
      " [-0.49815714  1.        ]]\n",
      "states [[ 0.17486764 -0.91881615]\n",
      " [ 0.87081929  0.27229882]]\n",
      "states [[ 0.38900979 -0.18825713]\n",
      " [-0.20505268  1.        ]]\n",
      "states [[ 0.79841188  1.        ]\n",
      " [ 1.         -0.74449673]]\n",
      "states [[ 0.13272871  1.        ]\n",
      " [-1.          0.72872509]]\n",
      "states [[-1.         -0.58806196]\n",
      " [-0.62175068  1.        ]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "states [[-1.         -1.        ]\n",
      " [-0.78760378 -0.53137896]]\n",
      "states [[ 0.9273409   1.        ]\n",
      " [-0.10023008  0.80025845]]\n",
      "states [[ 0.63856082  0.4469468 ]\n",
      " [-1.          0.07787267]]\n",
      "states [[ 0.49783431 -0.75690291]\n",
      " [-0.31208148 -0.38600056]]\n",
      "states [[ 1.         -0.07046008]\n",
      " [ 1.         -0.1956882 ]]\n",
      "states [[ 0.76142024 -1.        ]\n",
      " [-1.          0.45611232]]\n",
      "states [[0.83926515 0.40701379]\n",
      " [0.75106767 0.75931128]]\n",
      "states [[ 0.02284174  1.        ]\n",
      " [-0.11943764  0.70137659]]\n",
      "states [[ 0.65882913  1.        ]\n",
      " [-0.78208206 -0.31734582]]\n",
      "states [[-0.82185309 -0.20314967]\n",
      " [ 1.          0.31113011]]\n",
      "states [[-0.34160813 -0.76125706]\n",
      " [-0.39763271 -0.45812547]]\n",
      "states [[-0.77676717 -0.80965943]\n",
      " [ 1.          1.        ]]\n",
      "states [[-0.03711529 -1.        ]\n",
      " [ 0.53510412 -0.1163239 ]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "states [[-0.01996401 -0.1093686 ]\n",
      " [-0.28305931  0.30235628]]\n",
      "states [[-0.59632913  0.89536151]\n",
      " [-0.91853581  0.415484  ]]\n",
      "states [[-0.65920207 -0.94755181]\n",
      " [-0.14761418 -1.        ]]\n",
      "states [[ 0.94967503  0.69664083]\n",
      " [-0.05192081  0.44555428]]\n",
      "states [[ 0.59395228 -1.        ]\n",
      " [ 0.99270456 -1.        ]]\n",
      "states [[-1.          1.        ]\n",
      " [-0.46581787  1.        ]]\n",
      "states [[ 0.38823317  0.26017373]\n",
      " [-0.08434451  1.        ]]\n",
      "states [[-0.39305569 -0.41345749]\n",
      " [-0.54876322 -0.52922428]]\n",
      "states [[ 0.53935399  0.20691837]\n",
      " [ 0.55858594 -1.        ]]\n",
      "states [[ 0.38030042  0.188292  ]\n",
      " [ 0.38672719 -0.043187  ]]\n",
      "states [[-0.60620742  1.        ]\n",
      " [-0.11665276  0.83062499]]\n",
      "states [[-0.7360034  -1.        ]\n",
      " [ 0.17488533  0.940489  ]]\n",
      "states [[0.05789643 0.92781011]\n",
      " [0.21640054 0.59641616]]\n",
      "states [[-0.45176582 -1.        ]\n",
      " [-0.43823872 -0.16457302]]\n",
      "states [[-0.2429142   0.77113714]\n",
      " [ 1.          0.42146096]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "states [[ 0.22072759 -0.50392708]\n",
      " [-0.49799324 -0.48001018]]\n",
      "states [[-0.96140615  0.90264436]\n",
      " [ 1.         -0.52899941]]\n",
      "states [[0.26852543 1.        ]\n",
      " [1.         0.67211967]]\n",
      "states [[-0.62927688  0.55776582]\n",
      " [ 0.18144574 -0.28156177]]\n",
      "states [[-0.9663179   0.36495664]\n",
      " [-0.72018073  0.18746388]]\n",
      "states [[-1.          1.        ]\n",
      " [-0.53548347  0.15011442]]\n",
      "states [[ 0.01128592  0.7478208 ]\n",
      " [-0.91387563  1.        ]]\n",
      "states [[ 0.68310431  0.5207871 ]\n",
      " [ 0.52377569 -0.75620433]]\n",
      "states [[ 1.         -0.07140935]\n",
      " [ 1.         -0.64738817]]\n",
      "states [[0.98838111 0.91737231]\n",
      " [0.26925625 0.58013927]]\n",
      "states [[ 0.5755106  -1.        ]\n",
      " [ 0.37839093 -0.52938075]]\n",
      "states [[-0.19808826 -0.66637278]\n",
      " [ 1.          0.05345584]]\n",
      "states [[-0.67224352 -1.        ]\n",
      " [ 0.61602873  0.46515506]]\n",
      "states [[ 1.         -0.18095542]\n",
      " [-1.         -1.        ]]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "states [[-1.         -0.93542193]\n",
      " [ 0.79377116  0.19862597]]\n",
      "states [[ 0.55409472 -0.48146032]\n",
      " [-1.          0.3853554 ]]\n",
      "states [[ 1.         -0.28046572]\n",
      " [-1.          0.42449468]]\n",
      "states [[-1.          1.        ]\n",
      " [ 0.39292148 -0.37299325]]\n",
      "states [[0.57772107 0.4112811 ]\n",
      " [0.71562408 1.        ]]\n",
      "states [[-0.83748786 -0.52649067]\n",
      " [-0.31186046  0.48472888]]\n",
      "states [[-0.64477341  0.21687612]\n",
      " [ 0.22191222 -1.        ]]\n",
      "states [[-1.          0.27721571]\n",
      " [-0.41703385  1.        ]]\n",
      "states [[ 0.1707743  -0.12301881]\n",
      " [-0.07770909 -0.49109607]]\n",
      "states [[0.81545895 0.49289824]\n",
      " [0.94792819 1.        ]]\n",
      "states [[ 0.9991266   1.        ]\n",
      " [ 0.41861698 -0.13472527]]\n",
      "states [[-0.20007352  0.72106355]\n",
      " [ 0.94022805  0.94547567]]\n",
      "states [[ 1.        -1.       ]\n",
      " [-1.        -0.8726682]]\n",
      "states [[ 0.06065026  0.36971494]\n",
      " [ 1.         -0.95321803]]\n",
      "states [[-0.47571904  1.        ]\n",
      " [ 0.27494978 -0.47895462]]\n",
      "states [[-0.50356365  1.        ]\n",
      " [ 0.33344042 -1.        ]]\n",
      "states [[ 0.33321931  0.3887263 ]\n",
      " [ 1.         -0.71419755]]\n",
      "states [[ 0.2303579  -0.43011254]\n",
      " [ 0.91365176  0.13222034]]\n",
      "states [[-0.29465602 -0.94805201]\n",
      " [-0.37119492 -1.        ]]\n",
      "states [[ 1.         -0.48590211]\n",
      " [-1.         -0.19030796]]\n",
      "states [[-1.          0.12657217]\n",
      " [ 0.69041674 -1.        ]]\n",
      "states [[ 0.38391068 -0.57981129]\n",
      " [ 0.4769278  -0.63729011]]\n",
      "states [[-0.35520525 -0.27815566]\n",
      " [-0.90732155 -0.33360124]]\n",
      "states [[-0.00599267 -0.65256571]\n",
      " [ 0.01450394  0.71273988]]\n",
      "states [[-1.          1.        ]\n",
      " [-1.         -0.06316083]]\n",
      "states [[ 0.36486691  0.73116998]\n",
      " [-0.35248637 -0.49955538]]\n",
      "states [[ 1.          0.02675668]\n",
      " [-0.28918097 -0.12257714]]\n",
      "states [[ 0.99807676 -0.54681029]\n",
      " [ 0.04604023 -0.13026003]]\n",
      "states [[ 0.1399089  -1.        ]\n",
      " [-0.63602422 -0.15281618]]\n",
      "states [[-0.35080304  0.39277769]\n",
      " [-0.85594592 -0.28463892]]\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards\n",
    "        print(\"states\", actions)\n",
    "        # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.          -6.14030886  -1.5\n",
      "   -0.           0.          -7.11741829   5.97645617  -0.\n",
      "    0.          -7.19271708  -1.55886006 -10.52409172  -0.98100001\n",
      "   -7.11741829   5.85873604 -10.52409172  -0.98100001]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.          -7.9574213   -1.5\n",
      "    0.           0.           7.11741829   5.97645617   0.\n",
      "    0.         -10.52526093  -1.55886006 -25.67837906  -0.98100001\n",
      "    7.11741829   5.85873604 -25.67837906  -0.98100001]]\n",
      "[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [-6.140308856964111, -7.95742130279541], [-1.5, -1.5], [-0.0, 0.0], [0.0, 0.0], [-7.11741828918457, 7.11741828918457], [5.976456165313721, 5.976456165313721], [-0.0, 0.0], [0.0, 0.0], [-7.1927170753479, -10.525260925292969], [-1.5588600635528564, -1.5588600635528564], [-10.524091720581055, -25.67837905883789], [-0.9810000061988831, -0.9810000061988831], [-7.11741828918457, 7.11741828918457], [5.858736038208008, 5.858736038208008], [-10.524091720581055, -25.67837905883789], [-0.9810000061988831, -0.9810000061988831]]\n",
      "48\n",
      "tensor [tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,  -6.1403,  -1.5000,  -0.0000,   0.0000,  -7.1174,   5.9765,\n",
      "         -0.0000,   0.0000,  -7.1927,  -1.5589, -10.5241,  -0.9810,  -7.1174,\n",
      "          5.8587, -10.5241,  -0.9810]), tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,  -7.9574,  -1.5000,   0.0000,   0.0000,   7.1174,   5.9765,\n",
      "          0.0000,   0.0000, -10.5253,  -1.5589, -25.6784,  -0.9810,   7.1174,\n",
      "          5.8587, -25.6784,  -0.9810])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def transpose_list(mylist):\n",
    "    return list(map(list, zip(*mylist)))\n",
    "\n",
    "def transpose_to_tensorAsitis(input_list):\n",
    "    make_tensor = lambda x: torch.tensor(x, dtype=torch.float)\n",
    "    return list(map(make_tensor, input_list))\n",
    "\n",
    "env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "next_states = env_info.vector_observations\n",
    "print(next_states)\n",
    "s = transpose_list(next_states)\n",
    "print(s)\n",
    "p = np.concatenate((next_states[0], next_states[1]))\n",
    "print(np.shape(p)[0])\n",
    "s = transpose_to_tensorAsitis(next_states)\n",
    "print(\"tensor\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Number of agents: 2\n",
      "Size of each action: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1/10000   0% ETA:  4:21:41 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.009999999776482582 0\n",
      "agent1/mean_episode_rewards 0.0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 101/10000   1% ETA:  7:16:22 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.00399999987334013 100\n",
      "agent1/mean_episode_rewards -0.00399999987334013 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 201/10000   2% ETA:  7:13:43 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0036999998800456525 200\n",
      "agent1/mean_episode_rewards -0.0052999998815357685 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 301/10000   3% ETA:  7:07:51 |/                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004399999901652336 300\n",
      "agent1/mean_episode_rewards -0.005599999874830246 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 401/10000   4% ETA:  7:02:45 |/                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005199999883770943 400\n",
      "agent1/mean_episode_rewards -0.004799999892711639 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 501/10000   5% ETA:  6:57:55 |/                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0040999999083578586 500\n",
      "agent1/mean_episode_rewards -0.0058999998681247235 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 601/10000   6% ETA:  6:55:20 |//                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005199999883770943 600\n",
      "agent1/mean_episode_rewards -0.004799999892711639 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 701/10000   7% ETA:  6:50:27 |//                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0046999998949468135 700\n",
      "agent1/mean_episode_rewards -0.0052999998815357685 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 801/10000   8% ETA:  6:45:41 |///                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 800\n",
      "agent1/mean_episode_rewards -0.004599999897181988 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 901/10000   9% ETA:  6:41:00 |///                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 900\n",
      "agent1/mean_episode_rewards -0.005099999886006117 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1001/10000  10% ETA:  6:36:21 |///                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 1000\n",
      "agent1/mean_episode_rewards -0.005199999883770943 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1101/10000  11% ETA:  6:32:38 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004399999901652336 1100\n",
      "agent1/mean_episode_rewards -0.005599999874830246 1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1201/10000  12% ETA:  6:28:00 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004599999897181988 1200\n",
      "agent1/mean_episode_rewards -0.005399999879300594 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1301/10000  13% ETA:  6:23:26 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005099999886006117 1300\n",
      "agent1/mean_episode_rewards -0.0048999998904764655 1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1401/10000  14% ETA:  6:18:53 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004599999897181988 1400\n",
      "agent1/mean_episode_rewards -0.005399999879300594 1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1501/10000  15% ETA:  6:14:22 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 1500\n",
      "agent1/mean_episode_rewards -0.005099999886006117 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1601/10000  16% ETA:  6:10:38 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0052999998815357685 1600\n",
      "agent1/mean_episode_rewards -0.0046999998949468135 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1701/10000  17% ETA:  6:06:16 |//////                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0046999998949468135 1700\n",
      "agent1/mean_episode_rewards -0.0052999998815357685 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1801/10000  18% ETA:  6:02:22 |//////                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004999999888241291 1800\n",
      "agent1/mean_episode_rewards -0.004999999888241291 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1901/10000  19% ETA:  5:57:49 |///////                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 1900\n",
      "agent1/mean_episode_rewards -0.005199999883770943 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2001/10000  20% ETA:  5:53:17 |///////                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 2000\n",
      "agent1/mean_episode_rewards -0.005199999883770943 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2101/10000  21% ETA:  5:49:07 |///////                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005999999865889549 2100\n",
      "agent1/mean_episode_rewards -0.003999999910593033 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2201/10000  22% ETA:  5:44:36 |////////                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004999999888241291 2200\n",
      "agent1/mean_episode_rewards -0.004999999888241291 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2301/10000  23% ETA:  5:40:05 |////////                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.003999999910593033 2300\n",
      "agent1/mean_episode_rewards -0.005999999865889549 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2401/10000  24% ETA:  5:35:46 |////////                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 2400\n",
      "agent1/mean_episode_rewards -0.004599999897181988 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2501/10000  25% ETA:  5:31:35 |/////////                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004599999897181988 2500\n",
      "agent1/mean_episode_rewards -0.005399999879300594 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2601/10000  26% ETA:  5:27:05 |/////////                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0046999998949468135 2600\n",
      "agent1/mean_episode_rewards -0.0052999998815357685 2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2701/10000  27% ETA:  5:22:59 |/////////                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005199999883770943 2700\n",
      "agent1/mean_episode_rewards -0.004799999892711639 2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2801/10000  28% ETA:  5:18:28 |//////////                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005199999883770943 2800\n",
      "agent1/mean_episode_rewards -0.004799999892711639 2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2901/10000  29% ETA:  5:13:57 |//////////                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0036999999172985555 2900\n",
      "agent1/mean_episode_rewards -0.006299999859184027 2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3001/10000  30% ETA:  5:09:50 |///////////                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0040999999083578586 3000\n",
      "agent1/mean_episode_rewards -0.0058999998681247235 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3101/10000  31% ETA:  5:05:40 |///////////                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 3100\n",
      "agent1/mean_episode_rewards -0.004599999897181988 3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3201/10000  32% ETA:  5:01:28 |///////////                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004499999899417162 3200\n",
      "agent1/mean_episode_rewards -0.0054999998770654205 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3301/10000  33% ETA:  4:56:56 |////////////                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004399999901652336 3300\n",
      "agent1/mean_episode_rewards -0.005599999874830246 3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3401/10000  34% ETA:  4:52:25 |////////////                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004399999901652336 3400\n",
      "agent1/mean_episode_rewards -0.005599999874830246 3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3501/10000  35% ETA:  4:48:01 |////////////                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 3500\n",
      "agent1/mean_episode_rewards -0.004599999897181988 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3601/10000  36% ETA:  4:43:38 |/////////////                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0052999998815357685 3600\n",
      "agent1/mean_episode_rewards -0.0046999998949468135 3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3701/10000  37% ETA:  4:39:07 |/////////////                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 3700\n",
      "agent1/mean_episode_rewards -0.005199999883770943 3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3801/10000  38% ETA:  4:34:51 |//////////////                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005599999874830246 3800\n",
      "agent1/mean_episode_rewards -0.004399999901652336 3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3901/10000  39% ETA:  4:30:21 |//////////////                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004599999897181988 3900\n",
      "agent1/mean_episode_rewards -0.005399999879300594 3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4001/10000  40% ETA:  4:26:00 |//////////////                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005099999886006117 4000\n",
      "agent1/mean_episode_rewards -0.0048999998904764655 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4101/10000  41% ETA:  4:21:30 |///////////////                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004399999901652336 4100\n",
      "agent1/mean_episode_rewards -0.005599999874830246 4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4201/10000  42% ETA:  4:17:01 |///////////////                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004199999906122684 4200\n",
      "agent1/mean_episode_rewards -0.005799999870359898 4200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4301/10000  43% ETA:  4:12:41 |///////////////                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 4300\n",
      "agent1/mean_episode_rewards -0.004599999897181988 4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4401/10000  44% ETA:  4:08:11 |////////////////                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005599999874830246 4400\n",
      "agent1/mean_episode_rewards -0.004399999901652336 4400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4501/10000  45% ETA:  4:03:54 |////////////////                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005099999886006117 4500\n",
      "agent1/mean_episode_rewards -0.0048999998904764655 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4601/10000  46% ETA:  3:59:25 |/////////////////                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 4600\n",
      "agent1/mean_episode_rewards -0.004599999897181988 4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4701/10000  47% ETA:  3:54:56 |/////////////////                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004999999888241291 4700\n",
      "agent1/mean_episode_rewards -0.004999999888241291 4700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4801/10000  48% ETA:  3:50:27 |/////////////////                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 4800\n",
      "agent1/mean_episode_rewards -0.004599999897181988 4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4901/10000  49% ETA:  3:45:59 |//////////////////                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0052999998815357685 4900\n",
      "agent1/mean_episode_rewards -0.0046999998949468135 4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5001/10000  50% ETA:  3:41:38 |//////////////////                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0052999998815357685 5000\n",
      "agent1/mean_episode_rewards -0.0046999998949468135 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5101/10000  51% ETA:  3:37:09 |//////////////////                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 5100\n",
      "agent1/mean_episode_rewards -0.004599999897181988 5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5201/10000  52% ETA:  3:32:41 |///////////////////                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0046999998949468135 5200\n",
      "agent1/mean_episode_rewards -0.0052999998815357685 5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5301/10000  53% ETA:  3:28:17 |///////////////////                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005099999886006117 5300\n",
      "agent1/mean_episode_rewards -0.0048999998904764655 5300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5401/10000  54% ETA:  3:23:51 |///////////////////                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004599999897181988 5400\n",
      "agent1/mean_episode_rewards -0.005399999879300594 5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5501/10000  55% ETA:  3:19:28 |////////////////////                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004199999906122684 5500\n",
      "agent1/mean_episode_rewards -0.005799999870359898 5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5601/10000  56% ETA:  3:15:00 |////////////////////                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0052999998815357685 5600\n",
      "agent1/mean_episode_rewards -0.0046999998949468135 5600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5701/10000  57% ETA:  3:10:33 |/////////////////////                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0060999998636543755 5700\n",
      "agent1/mean_episode_rewards -0.003899999912828207 5700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5801/10000  58% ETA:  3:06:11 |/////////////////////                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004399999901652336 5800\n",
      "agent1/mean_episode_rewards -0.005599999874830246 5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5901/10000  59% ETA:  3:01:45 |/////////////////////                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 5900\n",
      "agent1/mean_episode_rewards -0.005099999886006117 5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6001/10000  60% ETA:  2:57:23 |//////////////////////               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 6000\n",
      "agent1/mean_episode_rewards -0.005199999883770943 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6101/10000  61% ETA:  2:52:55 |//////////////////////               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0046999998949468135 6100\n",
      "agent1/mean_episode_rewards -0.0052999998815357685 6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6201/10000  62% ETA:  2:48:28 |//////////////////////               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004399999901652336 6200\n",
      "agent1/mean_episode_rewards -0.005599999874830246 6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6301/10000  63% ETA:  2:44:04 |///////////////////////              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005699999872595072 6300\n",
      "agent1/mean_episode_rewards -0.0042999999038875105 6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6401/10000  64% ETA:  2:39:36 |///////////////////////              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005099999886006117 6400\n",
      "agent1/mean_episode_rewards -0.0048999998904764655 6400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6501/10000  65% ETA:  2:35:13 |////////////////////////             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004999999888241291 6500\n",
      "agent1/mean_episode_rewards -0.004999999888241291 6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6601/10000  66% ETA:  2:30:46 |////////////////////////             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005099999886006117 6600\n",
      "agent1/mean_episode_rewards -0.0048999998904764655 6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6701/10000  67% ETA:  2:26:22 |////////////////////////             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 6700\n",
      "agent1/mean_episode_rewards -0.005199999883770943 6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6801/10000  68% ETA:  2:21:54 |/////////////////////////            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004999999888241291 6800\n",
      "agent1/mean_episode_rewards -0.004999999888241291 6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6901/10000  69% ETA:  2:17:27 |/////////////////////////            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 6900\n",
      "agent1/mean_episode_rewards -0.005099999886006117 6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7001/10000  70% ETA:  2:12:59 |/////////////////////////            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.006199999861419201 7000\n",
      "agent1/mean_episode_rewards -0.003799999915063381 7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7101/10000  71% ETA:  2:08:36 |//////////////////////////           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005599999874830246 7100\n",
      "agent1/mean_episode_rewards -0.004399999901652336 7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7201/10000  72% ETA:  2:04:12 |//////////////////////////           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 7200\n",
      "agent1/mean_episode_rewards -0.005099999886006117 7200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7301/10000  73% ETA:  1:59:47 |///////////////////////////          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005199999883770943 7300\n",
      "agent1/mean_episode_rewards -0.004799999892711639 7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7401/10000  74% ETA:  1:55:23 |///////////////////////////          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004499999899417162 7400\n",
      "agent1/mean_episode_rewards -0.0054999998770654205 7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7501/10000  75% ETA:  1:50:55 |///////////////////////////          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 7500\n",
      "agent1/mean_episode_rewards -0.005099999886006117 7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7601/10000  76% ETA:  1:46:30 |////////////////////////////         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0040999999083578586 7600\n",
      "agent1/mean_episode_rewards -0.0058999998681247235 7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7701/10000  77% ETA:  1:42:03 |////////////////////////////         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 7700\n",
      "agent1/mean_episode_rewards -0.004599999897181988 7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7801/10000  78% ETA:  1:37:36 |////////////////////////////         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 7800\n",
      "agent1/mean_episode_rewards -0.005199999883770943 7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7901/10000  79% ETA:  1:33:12 |/////////////////////////////        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 7900\n",
      "agent1/mean_episode_rewards -0.005099999886006117 7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8001/10000  80% ETA:  1:28:45 |/////////////////////////////        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004999999888241291 8000\n",
      "agent1/mean_episode_rewards -0.004999999888241291 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8101/10000  81% ETA:  1:24:19 |/////////////////////////////        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 8100\n",
      "agent1/mean_episode_rewards -0.004599999897181988 8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8201/10000  82% ETA:  1:19:52 |//////////////////////////////       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004199999906122684 8200\n",
      "agent1/mean_episode_rewards -0.005799999870359898 8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8301/10000  83% ETA:  1:15:27 |//////////////////////////////       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005399999879300594 8300\n",
      "agent1/mean_episode_rewards -0.004599999897181988 8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8401/10000  84% ETA:  1:11:01 |///////////////////////////////      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005599999874830246 8400\n",
      "agent1/mean_episode_rewards -0.004399999901652336 8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8501/10000  85% ETA:  1:06:34 |///////////////////////////////      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004499999899417162 8500\n",
      "agent1/mean_episode_rewards -0.0054999998770654205 8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8601/10000  86% ETA:  1:02:08 |///////////////////////////////      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004199999906122684 8600\n",
      "agent1/mean_episode_rewards -0.005799999870359898 8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8701/10000  87% ETA:  0:57:42 |////////////////////////////////     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0054999998770654205 8700\n",
      "agent1/mean_episode_rewards -0.004499999899417162 8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8801/10000  88% ETA:  0:53:15 |////////////////////////////////     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005599999874830246 8800\n",
      "agent1/mean_episode_rewards -0.004399999901652336 8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8901/10000  89% ETA:  0:48:48 |////////////////////////////////     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004999999888241291 8900\n",
      "agent1/mean_episode_rewards -0.004999999888241291 8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9001/10000  90% ETA:  0:44:22 |/////////////////////////////////    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0054999998770654205 9000\n",
      "agent1/mean_episode_rewards -0.004499999899417162 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9101/10000  91% ETA:  0:39:55 |/////////////////////////////////    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0042999999038875105 9100\n",
      "agent1/mean_episode_rewards -0.005699999872595072 9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9201/10000  92% ETA:  0:35:29 |//////////////////////////////////   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0042999999038875105 9200\n",
      "agent1/mean_episode_rewards -0.005699999872595072 9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9301/10000  93% ETA:  0:31:02 |//////////////////////////////////   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0048999998904764655 9300\n",
      "agent1/mean_episode_rewards -0.005099999886006117 9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9401/10000  94% ETA:  0:26:36 |//////////////////////////////////   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005099999886006117 9400\n",
      "agent1/mean_episode_rewards -0.0048999998904764655 9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9501/10000  95% ETA:  0:22:10 |///////////////////////////////////  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004799999892711639 9500\n",
      "agent1/mean_episode_rewards -0.005199999883770943 9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9601/10000  96% ETA:  0:17:43 |///////////////////////////////////  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005699999872595072 9600\n",
      "agent1/mean_episode_rewards -0.0042999999038875105 9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9701/10000  97% ETA:  0:13:16 |///////////////////////////////////  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0054999998770654205 9700\n",
      "agent1/mean_episode_rewards -0.004499999899417162 9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9801/10000  98% ETA:  0:08:50 |//////////////////////////////////// | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.004599999897181988 9800\n",
      "agent1/mean_episode_rewards -0.005399999879300594 9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9901/10000  99% ETA:  0:04:23 |//////////////////////////////////// | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.0046999998949468135 9900\n",
      "agent1/mean_episode_rewards -0.0052999998815357685 9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 10000/10000 100% Time: 7:24:20 |||||||||||||||||||||||||||||||||||||| \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards -0.005050504937617465 9999\n",
      "agent1/mean_episode_rewards -0.004949494838865117 9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "#import envs\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "#from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "\n",
    "# keep training awake\n",
    "from workspace_utils import keep_awake\n",
    "\n",
    "# for saving gif\n",
    "import imageio\n",
    "\n",
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def pre_process(entity, batchsize):\n",
    "    processed_entity = []\n",
    "    for j in range(3):\n",
    "        list = []\n",
    "        for i in range(batchsize):\n",
    "            b = entity[i][j]\n",
    "            list.append(b)\n",
    "        c = torch.Tensor(list)\n",
    "        processed_entity.append(c)\n",
    "    return processed_entity\n",
    "\n",
    "\n",
    "def main():\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "\n",
    "    # examine the state space \n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    \n",
    "    seeding()\n",
    "    # number of parallel agents\n",
    "    #parallel_envs = num_agents\n",
    "    # number of training episodes.\n",
    "    # change this to higher number to experiment. say 30000.\n",
    "    \n",
    "    number_of_episodes = 10000\n",
    "    episode_length = 100\n",
    "    batchsize = 125\n",
    "    # how many episodes to save policy and gif\n",
    "    save_interval = 1000\n",
    "    t = 0\n",
    "    \n",
    "    # amplitude of OU noise\n",
    "    # this slowly decreases to 0\n",
    "    noise = 2.0\n",
    "    noise_reduction = 0.9999\n",
    "\n",
    "    # how many episodes before update\n",
    "    episode_per_update =  1\n",
    "    no_of_updates_perTime = 2\n",
    "\n",
    "    log_path = os.getcwd()+\"/log\"\n",
    "    model_dir= os.getcwd()+\"/model_dir\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    #torch.set_num_threads(parallel_envs)\n",
    "    #env = envs.make_parallel_env(parallel_envs)\n",
    "    \n",
    "    # keep 5000 episodes worth of replay\n",
    "    buffer = ReplayBuffer(int(500*episode_length))\n",
    "    \n",
    "    # initialize policy and critic\n",
    "    maddpg = MADDPG()\n",
    "    #logger = SummaryWriter(log_dir=log_path)\n",
    "    agent0_reward = []\n",
    "    agent1_reward = []\n",
    "    #agent2_reward = []\n",
    "\n",
    "    # training loop\n",
    "    # show progressbar\n",
    "    import progressbar as pb\n",
    "    widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "              pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "    \n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "\n",
    "    # use keep_awake to keep workspace from disconnecting\n",
    "    for episode in range(0, number_of_episodes):\n",
    "\n",
    "        timer.update(episode)\n",
    "\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        reward_this_episode = np.zeros((1, num_agents))\n",
    "        \n",
    "        #all_obs = env.reset() #\n",
    "        obs = states\n",
    "        obs_full = np.concatenate((states[0], states[1]))\n",
    "\n",
    "        #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "        # save info or not\n",
    "        save_info = ((episode) % save_interval < 1 or episode==number_of_episodes- 1)\n",
    "        tmax = 0\n",
    "        \n",
    "        #resetting noise \n",
    "        for i in range(num_agents):\n",
    "            maddpg.maddpg_agent[i].noise.reset()\n",
    "            \n",
    "        for episode_t in range(episode_length):\n",
    "\n",
    "            t += 1\n",
    "            \n",
    "\n",
    "            # explore = only explore for a certain number of episodes\n",
    "            # action input needs to be transposed\n",
    "            actions = maddpg.act(transpose_to_tensorAsitis(obs), noise=noise)\n",
    "            noise *= noise_reduction\n",
    "            \n",
    "            actions_array = torch.stack(actions).cpu().detach().numpy()\n",
    "            \n",
    "            # transpose the list of list\n",
    "            # flip the first two indices\n",
    "            # input to step requires the first index to correspond to number of parallel agents\n",
    "            actions_for_env = np.rollaxis(actions_array,1)\n",
    "            \n",
    "            # step forward one frame\n",
    "            env_info = env.step(actions_for_env)[brain_name]\n",
    "            \n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards\n",
    "            \n",
    "            rewards_for_env = np.hstack(rewards)\n",
    "            \n",
    "            obs = states\n",
    "            obs_full = np.concatenate((states[0], states[1]))\n",
    "            next_obs = next_states\n",
    "            next_obs_full = np.concatenate((next_states[0], next_states[1]))\n",
    "            # add data to buffer\n",
    "            transition = (np.array([obs]), np.array([obs_full]), np.array([actions_for_env]), np.array([rewards_for_env]), np.array([next_obs]), np.array([next_obs_full]), np.array([dones] , dtype = 'float'))\n",
    "            buffer.push(transition)\n",
    "            \n",
    "            reward_this_episode += rewards\n",
    "\n",
    "            obs, obs_full = next_obs, next_obs_full\n",
    "            \n",
    "            # update once after every episode_per_update\n",
    "            if len(buffer) > batchsize and episode % episode_per_update == 0:\n",
    "                for _ in range(no_of_updates_perTime):\n",
    "                    for a_i in range(num_agents):\n",
    "                        samples = buffer.sample(batchsize)\n",
    "                        #updating the weights of the n/w\n",
    "                        maddpg.update(samples, a_i)\n",
    "                    maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "                \n",
    "            if np.any(dones):\n",
    "                # if the episode is done the loop is break to the next episode\n",
    "                break\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            agent0_reward.append(reward_this_episode[0,0])\n",
    "            agent1_reward.append(reward_this_episode[0,1])\n",
    "            \n",
    "\n",
    "        if episode % 100 == 0 or episode == number_of_episodes-1:\n",
    "            avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward)]\n",
    "            agent0_reward = []\n",
    "            agent1_reward = []\n",
    "            for a_i, avg_rew in enumerate(avg_rewards):\n",
    "                #logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "                print('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "\n",
    "        #saving model\n",
    "        save_dict_list =[]\n",
    "        if save_info:\n",
    "            for i in range(num_agents):\n",
    "\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "\n",
    "                torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "                \n",
    "            # save gif files\n",
    "            #imageio.mimsave(os.path.join(model_dir, 'episode-{}.gif'.format(episode)), \n",
    "                            #frames, duration=.04)\n",
    "    timer.finish()\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 6 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3]])\n",
    "b = np.array([[0,4,5]])\n",
    "c = np.concatenate(a+b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
